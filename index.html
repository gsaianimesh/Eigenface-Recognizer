<!DOCTYPE html>
<html lang="en" class="dark"> <!-- Hard-coded to dark mode -->
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AURA: Eigenface Recognition</title>
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Inter Font -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800;900&display=swap" rel="stylesheet">
    
    <style>
        /* ... existing styles ... */
        body {
            font-family: 'Inter', sans-serif;
        }
        
        #plexus-bg {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -10;
        }
        
        input[type="file"]::file-selector-button {
            @apply font-semibold bg-indigo-600 text-white border-0 px-4 py-2 rounded-lg cursor-pointer transition-colors duration-200 hover:bg-indigo-700;
        }
        
        .loader {
            @apply border-4 border-slate-700 border-t-indigo-500 rounded-full w-10 h-10 animate-spin mx-auto my-5;
        }
        
        html {
            scroll-behavior: smooth;
        }
        
        .fade-in-up {
            animation: fadeInUp 0.6s ease-out forwards;
            opacity: 0;
        }
        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
        
        /* Ensure video element isn't tiny */
        #videoElement {
            transform: scaleX(-1); /* Mirror the video */
        }
    </style>
</head>
<body class="bg-slate-950 text-slate-200 transition-colors duration-300 overflow-x-hidden min-h-screen">
    
    <!-- Plexus Background Canvas -->
    <canvas id="plexus-bg"></canvas>
    
    <!-- === Header === -->
    <nav class="w-full max-w-7xl mx-auto p-6 flex justify-between items-center sticky top-0 z-50 bg-slate-950/70 backdrop-blur-xl border-b border-indigo-500/30">
        <!-- ... existing header ... -->
         <h1 class="text-2xl font-bold text-indigo-400 drop-shadow-[0_0_10px_rgba(129,140,248,0.5)]">
            AURA
        </h1>
    </nav>
    
    <!-- === NEW: Camera Modal === -->
    <div id="cameraModal" class="hidden fixed inset-0 z-[100] flex items-center justify-center bg-black/80 backdrop-blur-md">
        <div class="bg-slate-900 border border-indigo-500/30 rounded-2xl shadow-xl p-6 w-full max-w-lg">
            <h3 class="text-xl font-bold text-white text-center mb-4">Live Camera Feed</h3>
            <video id="videoElement" width="600" height="450" autoplay playsinline class="rounded-lg w-full h-auto border border-slate-700"></video>
            <div class="flex gap-4 mt-6">
                <button id="snapButton"
                        class="w-full flex items-center justify-center gap-2 bg-gradient-to-r from-indigo-500 to-indigo-600 text-white font-bold py-3 px-4 rounded-lg shadow-lg shadow-indigo-600/50 hover:shadow-indigo-500/50 hover:from-indigo-600 hover:to-indigo-700 hover:-translate-y-1 transform transition-all duration-300 focus:outline-none focus:ring-4 focus:ring-offset-2 focus:ring-indigo-500 focus:ring-offset-slate-950">
                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-6 h-6">
                        <path stroke-linecap="round" stroke-linejoin="round" d="M6.827 6.175A2.31 2.31 0 015.186 7.23c-.38.054-.757.112-1.134.175C2.999 7.58 2.25 8.507 2.25 9.574V18a2.25 2.25 0 002.25 2.25h15A2.25 2.25 0 0021.75 18V9.574c0-1.067-.75-1.994-1.802-2.169a47.865 47.865 0 00-1.134-.175 2.31 2.31 0 01-1.64-1.055l-.822-1.316a2.192 2.192 0 00-1.736-1.039 48.774 48.774 0 00-5.232 0 2.192 2.192 0 00-1.736 1.039l-.821 1.316z" />
                        <path stroke-linecap="round" stroke-linejoin="round" d="M16.5 12.75a4.5 4.5 0 11-9 0 4.5 4.5 0 019 0zM18.75 10.5h.008v.008h-.008V10.5z" />
                    </svg>
                    Take Snapshot
                </button>
                <button id="closeCameraButton"
                        class="w-1/3 bg-gradient-to-r from-slate-500 to-slate-600 text-white font-bold py-3 px-4 rounded-lg shadow-lg shadow-slate-600/50 hover:shadow-slate-500/50 hover:from-slate-600 hover:to-slate-700 hover:-translate-y-1 transform transition-all duration-300 focus:outline-none focus:ring-4 focus:ring-offset-2 focus:ring-slate-500 focus:ring-offset-slate-950">
                    Close
                </button>
            </div>
        </div>
    </div>
    <!-- === END: Camera Modal === -->

    <!-- === Main Content === -->
    <main class="w-full max-w-7xl mx-auto p-6">
        <div class="flex flex-col lg:flex-row gap-8 w-full">
            
            <!-- === Left Column: Interactive === -->
            <div class="lg:w-1/2 flex flex-col gap-8 fade-in-up" style="animation-delay: 200ms;">
                
                <!-- Card 1: Face Recognizer -->
                <div class="bg-slate-900/70 backdrop-blur-xl p-8 rounded-2xl shadow-xl border border-indigo-500/30 transition-all duration-300">
                    <h2 class="text-3xl font-bold text-center text-white drop-shadow-[0_0_8px_rgba(255,255,255,0.3)] mb-6">
                        AURA Face Recognition
                    </h2>
                    
                    <div class="mb-6">
                        <label class="block mb-2 text-sm font-medium text-slate-300" for="imageUploader">
                            Upload an image for verification:
                        </label>
                        <div class="flex gap-2">
                            <input type="file" id="imageUploader" accept="image/*"
                                   class="block w-full text-sm text-slate-300 file:mr-4 file:py-2 file:px-4
                                          file:rounded-lg file:border-0 file:text-sm file:font-semibold
                                          file:bg-indigo-900/50 file:text-indigo-300 hover:file:bg-indigo-900
                                          bg-slate-800 rounded-lg border border-slate-700 cursor-pointer focus:outline-none
                                          focus:ring-2 focus:ring-indigo-500 focus:border-indigo-500">
                            <!-- === NEW: Use Camera Button === -->
                            <button id="useCameraButton"
                                    class="flex-shrink-0 flex items-center justify-center gap-2 bg-gradient-to-r from-slate-500 to-slate-600 text-white p-3 rounded-lg shadow-lg shadow-slate-600/50 hover:shadow-slate-500/50 hover:from-slate-600 hover:to-slate-700 hover:-translate-y-1 transform transition-all duration-300 focus:outline-none focus:ring-4 focus:ring-offset-2 focus:ring-slate-500 focus:ring-offset-slate-950"
                                    title="Use Camera">
                                <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-6 h-6">
                                    <path stroke-linecap="round" stroke-linejoin="round" d="M6.827 6.175A2.31 2.31 0 015.186 7.23c-.38.054-.757.112-1.134.175C2.999 7.58 2.25 8.507 2.25 9.574V18a2.25 2.25 0 002.25 2.25h15A2.25 2.25 0 0021.75 18V9.574c0-1.067-.75-1.994-1.802-2.169a47.865 47.865 0 00-1.134-.175 2.31 2.31 0 01-1.64-1.055l-.822-1.316a2.192 2.192 0 00-1.736-1.039 48.774 48.774 0 00-5.232 0 2.192 2.192 0 00-1.736 1.039l-.821 1.316z" />
                                    <path stroke-linecap="round" stroke-linejoin="round" d="M16.5 12.75a4.5 4.5 0 11-9 0 4.5 4.5 0 019 0zM18.75 10.5h.008v.008h-.008V10.5z" />
                                </svg>
                            </button>
                        </div>
                    </div>
                    
                    <!-- ... existing single image preview ... -->
                    <div id="imagePreviewContainer" class="hidden mb-6 text-center">
                        <h4 class="text-sm font-medium text-slate-300 mb-2">Image Preview</h4>
                        <img id="imagePreview" src="" alt="Image Preview"
                             class="w-1/2 h-auto object-cover rounded-lg shadow-md border border-slate-700 mx-auto">
                    </div>
                    
                    <button id="recognizeButton"
                            class="w-full flex items-center justify-center gap-2 bg-gradient-to-r from-indigo-500 to-indigo-600 text-white font-bold py-3 px-4 rounded-lg shadow-lg shadow-indigo-600/50 hover:shadow-indigo-500/50 hover:from-indigo-600 hover:to-indigo-700 hover:-translate-y-1 transform transition-all duration-300 focus:outline-none focus:ring-4 focus:ring-offset-2 focus:ring-indigo-500 focus:ring-offset-slate-950">
                        <!-- ... existing button icon ... -->
                        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-6 h-6">
                            <path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z" />
                        </svg>
                        Initiate Scan
                    </button>
                    
                    <!-- ... existing loader, error, and results ... -->
                    <div id="loader" class="loader hidden"></div>
                    <div id="error" class="hidden mt-4 p-4 bg-red-900/30 text-red-300 border border-red-700 rounded-lg"></div>
                    <div id="results" class="hidden mt-8">
                        <h3 id="resultText" class="text-2xl font-semibold text-center text-green-400 mb-6"></h3>
                        <div class="flex flex-col md:flex-row gap-6">
                            <div class="flex-1 text-center">
                                <h4 class="text-lg font-medium text-slate-300 mb-2">Input Scan</h4>
                                <img id="testImage" src="" alt="Test Image"
                                     class="w-full h-auto object-cover rounded-lg shadow-md border border-slate-700">
                            </div>
                            <div class="flex-1 text-center">
                                <h4 class="text-lg font-medium text-slate-300 mb-2">Database Match</h4>
                                <img id="matchImage" src="" alt="Matched Image"
                                     class="w-full h-auto object-cover rounded-lg shadow-md border border-slate-700">
                            </div>
                        </div>
                        <div class="mt-8 pt-6 border-t border-indigo-500/30">
                             <h4 class="text-xl font-semibold text-white mb-3 text-center">
                                Scan Analysis
                            </h4>
                            <ol class="list-decimal list-inside space-y-2 text-slate-400">
                                <li>The image was flattened into a <strong>10,000-dimensional vector</strong>.</li>
                                <li>The system's <strong>"Mean Face" vector</strong> was subtracted from it.</li>
                                <li>This "difference" vector was <strong>projected onto the Eigenface subspace</strong>.</li>
                                <li>This created a <strong>50-point "signature"</strong> vector.</li>
                                <li>The <strong>Euclidean distance</strong> was calculated against all known database signatures.</li>
                                <li>The resulting match is the identity with the <strong>smallest distance value</strong>.</li>
                            </ol>
                        </div>
                    </div>
                </div>

                <!-- ... existing Card 2: Add Person ... -->
                <div class="bg-slate-900/70 backdrop-blur-xl p-8 rounded-2xl shadow-xl border border-indigo-500/30 transition-all duration-300 fade-in-up" style="animation-delay: 300ms;">
                    <h3 class="text-2xl font-bold text-center text-white drop-shadow-[0_0_8px_rgba(255,255,255,0.3)] mb-6">
                        Expand Database
                    </h3>

                    <div class="mb-6">
                        <label class="block mb-2 text-sm font-medium text-slate-300" for="personName">
                            Identity Name:
                        </label>
                        <input type="text" id="personName"
                               class="block w-full text-sm text-white bg-slate-800 rounded-lg border border-slate-700 p-2.5 focus:outline-none focus:ring-2 focus:ring-indigo-500"
                               placeholder="e.g., Subject 7">
                    </div>

                    <div class="mb-6">
                        <label class="block mb-2 text-sm font-medium text-slate-300" for="newImageUploader">
                            Upload reference images (select multiple):
                        </label>
                        <input type="file" id="newImageUploader" accept="image/*" multiple
                               class="block w-full text-sm text-slate-300 file:mr-4 file:py-2 file:px-4
                                      file:rounded-lg file:border-0 file:text-sm file:font-semibold
                                      file:bg-indigo-900/50 file:text-indigo-300 hover:file:bg-indigo-900
                                      bg-slate-800 rounded-lg border border-slate-700 cursor-pointer focus:outline-none
                                      focus:ring-2 focus:ring-indigo-500 focus:border-indigo-500">
                    </div>

                    <!-- ... existing Multi Image Preview ... -->
                    <div id="newImagePreviewContainer" class="hidden mb-6">
                        <div class="flex justify-between items-center mb-2">
                             <h4 id="newImageQueueHeader" class="text-sm font-medium text-slate-300">Image Queue (0):</h4>
                             <button id="clearQueueButton" class="text-xs text-red-400 hover:text-red-300">&times; Clear All</button>
                        </div>
                        <div id="newImageGallery" class="flex flex-wrap gap-2 p-2 bg-slate-800 rounded-lg border border-slate-700 min-h-[72px]">
                            <!-- Thumbnails will be injected here -->
                        </div>
                    </div>


                    <button id="addPersonButton"
                            class="w-full flex items-center justify-center gap-2 bg-gradient-to-r from-green-500 to-green-600 text-white font-bold py-3 px-4 rounded-lg shadow-lg shadow-green-600/50 hover:shadow-green-500/50 hover:from-green-600 hover:to-green-700 hover:-translate-y-1 transform transition-all duration-300 focus:outline-none focus:ring-4 focus:ring-offset-2 focus:ring-green-500 focus:ring-offset-slate-950">
                        <!-- ... existing button icon ... -->
                        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-6 h-6">
                          <path stroke-linecap="round" stroke-linejoin="round" d="M12 4.5v15m7.5-7.5h-15" />
                        </svg>
                        Add Identity & Retrain Model
                    </button>

                    <div id="addLoader" class="loader hidden"></div>
                    <div id="addMessage" class="hidden mt-4 p-4 rounded-lg text-center font-medium"></div>
                </div>
            </div>

            <!-- ... existing Right Column: Visualization ... -->
            <div class="lg:w-1/2 flex flex-col gap-8 fade-in-up" style="animation-delay: 400ms;">
                <div class="bg-slate-900/70 backdrop-blur-xl p-8 rounded-2xl shadow-xl border border-indigo-500/30 transition-all duration-300">
                    <h2 class="text-3xl font-bold text-center text-white drop-shadow-[0_0_8px_rgba(255,255,255,0.3)] mb-4">
                        How AURA Works
                    </h2>
                    <p class="text-center text-slate-400 mb-6">
                        This section reveals the core technology. AURA uses <strong>Principal Component Analysis (PCA)</strong>—a matrix algorithm—to find the most important features of a face, known as <strong>Eigenvectors</strong>.
                    </p>

                    <div class="flex flex-col gap-4">
                        <button id="loadDetailsButton"
                                class="w-full flex items-center justify-center gap-2 bg-gradient-to-r from-indigo-500 to-indigo-600 text-white font-bold py-3 px-4 rounded-lg shadow-lg shadow-indigo-600/50 hover:shadow-indigo-500/50 hover:from-indigo-600 hover:to-indigo-700 hover:-translate-y-1 transform transition-all duration-300 focus:outline-none focus:ring-4 focus:ring-offset-2 focus:ring-indigo-500 focus:ring-offset-slate-950">
                            <!-- ... existing button icon ... -->
                            <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-6 h-6">
                              <path stroke-linecap="round" stroke-linejoin="round" d="M3.75 12.75l.938 1.125m0 0l3.094 3.713L18 6.75l-9.281 9.281-3.094-3.713z" />
                            </svg>
                            Show Model Internals
                        </button>
                        
                        <a href="YOUR_PROJECT_REPORT.pdf" target="_blank" rel="noopener noreferrer"
                           class="block w-full flex items-center justify-center gap-2 bg-gradient-to-r from-slate-500 to-slate-600 text-white font-bold py-3 px-4 rounded-lg shadow-lg shadow-slate-600/50 hover:shadow-slate-500/50 hover:from-slate-600 hover:to-slate-700 hover:-translate-y-1 transform transition-all duration-300 focus:outline-none focus:ring-4 focus:ring-offset-2 focus:ring-slate-500 focus:ring-offset-slate-950 text-center">
                           <!-- ... existing button icon ... -->
                           <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-6 h-6">
                              <path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 00-3.375-3.375h-1.5A1.125 1.125 0 0113.5 7.125v-1.5a3.375 3.375 0 00-3.375-3.375H8.25m.75 12l3 3m0 0l3-3m-3 3v-6m-1.5-9H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 00-9-9z" />
                            </svg>
                           View Technology Brief (PDF)
                        </a>
                    </div>
                    
                    <div id="detailsLoader" class="loader hidden"></div>
                    <div id="detailsError" class="hidden mt-4 p-4 bg-red-900/30 text-red-300 border border-red-700 rounded-lg"></div>

                    <!-- ... existing Details Results Section ... -->
                    <div id="detailsResults" class="hidden mt-8 space-y-8">
                        
                        <!-- Step 1: Mean Face -->
                        <div class="text-center">
                            <h3 class="text-xl font-semibold text-white mb-2">1. The "Mean Face" (Data Centering)</h3>
                            <p class="text-slate-400 mb-4 text-sm">
                                The average of all faces in the database. This "mean vector" is subtracted from every face, <strong>centering the data</strong>, which is a crucial first step for analysis.
                            </p>
                            <img id="meanFaceImage" src="" alt="Mean Face"
                                 class="w-1/2 h-auto object-cover rounded-lg shadow-md border border-slate-700 mx-auto">
                        </div>

                        <!-- Step 2: Eigenfaces -->
                        <div class="text-center pt-8 border-t border-indigo-500/30">
                            <h3 class="text-xl font-semibold text-white mb-2">2. The "Eigenfaces" (Principal Eigenvectors)</h3>
                            <p class="text-slate-400 mb-4 text-sm">
                                These are the principal <strong>Eigenvectors</strong> of the database, sorted by their corresponding <strong>Eigenvalue</strong>. They represent the most significant patterns of variation found in human faces.
                            </p>
                            <div id="eigenfacesContainer" class="flex flex-wrap justify-center gap-4">
                                <!-- Eigenfaces will be injected here by JS -->
                            </div>
                        </div>

                        <!-- Step 3: Projection -->
                        <div class="text-center pt-8 border-t border-indigo-500/30">
                            <h3 class="text-xl font-semibold text-white mb-2">3. The Recognition Process (Projection)</h3>
                            <p class="text-slate-400 mb-4 text-sm">
                                Recognition is a <strong>change of basis</strong>. We project a new face vector onto this "face space" (spanned by the Eigenvectors) by calculating its <strong>dot product</strong> with each. This creates a small "signature" vector, which we compare using <strong>Euclidean distance</strong>.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </main>
    
    <!-- ... existing footer ... -->
    <footer class="w-full max-w-7xl mx-auto p-6 text-center text-slate-400 text-sm">
        AURA Recognition System © 2025
    </footer>

    <script>
        // === [1] ANIMATED PLEXUS BACKGROUND LOGIC ===
        
        window.onload = function() {
            // ... existing plexus background code ...
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                const canvas = document.getElementById('plexus-bg');
                if (!canvas) {
                    console.error("Canvas element not found");
                    return;
                }
                const ctx = canvas.getContext('2d');
                let particles = [];
                let width = window.innerWidth;
                let height = document.body.scrollHeight; // Use scrollHeight to cover full page
                canvas.width = width;
                canvas.height = height;
                
                const numParticles = Math.floor((width * height) / 25000); // Responsive particle count
                const maxDistance = 140; // Max line distance
                
                class Particle {
                    constructor() {
                        this.x = Math.random() * width;
                        this.y = Math.random() * height;
                        this.vx = Math.random() * 0.4 - 0.2; // slow movement
                        this.vy = Math.random() * 0.4 - 0.2;
                        this.radius = Math.random() * 1.5 + 0.5;
                    }
                    
                    update() {
                        this.x += this.vx;
                        this.y += this.vy;
                        
                        // Wrap around edges
                        if (this.x < 0) this.x = width;
                        if (this.x > width) this.x = 0;
                        if (this.y < 0) this.y = height;
                        if (this.y > height) this.y = 0;
                    }
                    
                    draw() {
                        ctx.beginPath();
                        ctx.arc(this.x, this.y, this.radius, 0, Math.PI * 2);
                        ctx.fillStyle = 'rgba(165, 180, 252, 0.5)'; // Lighter Indigo
                        ctx.fill();
                    }
                }
                
                function init() {
                    particles = [];
                    for (let i = 0; i < numParticles; i++) {
                        particles.push(new Particle());
                    }
                }
                
                function connect() {
                    for (let a = 0; a < particles.length; a++) {
                        for (let b = a + 1; b < particles.length; b++) {
                            const dx = particles[a].x - particles[b].x;
                            const dy = particles[a].y - particles[b].y;
                            const distance = Math.sqrt(dx * dx + dy * dy);
                            
                            if (distance < maxDistance) {
                                const opacity = 1 - (distance / maxDistance);
                                ctx.strokeStyle = `rgba(129, 140, 248, ${opacity * 0.3})`;
                                ctx.lineWidth = 0.5;
                                ctx.beginPath();
                                ctx.moveTo(particles[a].x, particles[a].y);
                                ctx.lineTo(particles[b].x, particles[b].y);
                                ctx.stroke();
                            }
                        }
                    }
                }
                
                function animate() {
                    ctx.clearRect(0, 0, width, height);
                    for (let particle of particles) {
                        particle.update();
                        particle.draw();
                    }
                    connect();
                    requestAnimationFrame(animate);
                }
                
                // Debounce resize
                let resizeTimer;
                window.addEventListener('resize', () => {
                    clearTimeout(resizeTimer);
                    resizeTimer = setTimeout(() => {
                        width = window.innerWidth;
                        height = document.body.scrollHeight;
                        canvas.width = width;
                        canvas.height = height;
                        init(); // Re-initialize particles on resize
                    }, 250);
                });
                
                init();
                animate();
            }
        };
        
        // === [2] EIGENFACE APP LOGIC ===
        
        // === Global Variables ===
        let fileToScan = null; // Holds the file (from upload OR camera)
        let newFilesList = []; // Global list for queued images
        let cameraStream = null; // To hold the active camera stream

        // === Recognizer Elements ===
        const uploader = document.getElementById('imageUploader');
        const button = document.getElementById('recognizeButton');
        const loader = document.getElementById('loader');
        const resultsDiv = document.getElementById('results');
        const errorDiv = document.getElementById('error');
        const resultText = document.getElementById('resultText');
        const testImage = document.getElementById('testImage');
        const matchImage = document.getElementById('matchImage');
        const imagePreviewContainer = document.getElementById('imagePreviewContainer');
        const imagePreview = document.getElementById('imagePreview');
        
        // === Camera Elements ===
        const useCameraButton = document.getElementById('useCameraButton');
        const cameraModal = document.getElementById('cameraModal');
        const videoElement = document.getElementById('videoElement');
        const snapButton = document.getElementById('snapButton');
        const closeCameraButton = document.getElementById('closeCameraButton');

        // === Add Person Elements ===
        const personNameInput = document.getElementById('personName');
        const newImageUploader = document.getElementById('newImageUploader');
        const addPersonButton = document.getElementById('addPersonButton');
        const addLoader = document.getElementById('addLoader');
        const addMessage = document.getElementById('addMessage');
        const newImagePreviewContainer = document.getElementById('newImagePreviewContainer');
        const newImageGallery = document.getElementById('newImageGallery');
        const newImageQueueHeader = document.getElementById('newImageQueueHeader');
        const clearQueueButton = document.getElementById('clearQueueButton');

        // === Model Details Elements ===
        const loadDetailsButton = document.getElementById('loadDetailsButton');
        // ... (rest of details elements)
        const detailsLoader = document.getElementById('detailsLoader');
        const detailsError = document.getElementById('detailsError');
        const detailsResults = document.getElementById('detailsResults');
        const meanFaceImage = document.getElementById('meanFaceImage');
        const eigenfacesContainer = document.getElementById('eigenfacesContainer');

        // === Recognizer Logic (with Camera) ===
        
        // 1. From File Uploader
        uploader.addEventListener('change', () => {
            const file = uploader.files[0];
            if (file) {
                fileToScan = file; // Set the global file
                const reader = new FileReader();
                reader.onload = (e) => {
                    imagePreview.src = e.target.result;
                    imagePreviewContainer.classList.remove('hidden');
                };
                reader.readAsDataURL(file);
            } else {
                fileToScan = null;
                imagePreviewContainer.classList.add('hidden');
            }
        });

        // 2. From Camera
        async function startCamera() {
            try {
                cameraStream = await navigator.mediaDevices.getUserMedia({ video: true });
                videoElement.srcObject = cameraStream;
                cameraModal.classList.remove('hidden');
            } catch (err) {
                console.error("Error accessing camera:", err);
                showError("Could not access camera. Please check permissions.");
            }
        }
        
        function stopCamera() {
            if (cameraStream) {
                cameraStream.getTracks().forEach(track => track.stop());
                cameraStream = null;
            }
            cameraModal.classList.add('hidden');
        }

        useCameraButton.addEventListener('click', startCamera);
        closeCameraButton.addEventListener('click', stopCamera);
        
        snapButton.addEventListener('click', () => {
            const canvas = document.createElement('canvas');
            canvas.width = videoElement.videoWidth;
            canvas.height = videoElement.videoHeight;
            const ctx = canvas.getContext('2d');
            
            // Flip the canvas horizontally to match the mirrored video
            ctx.translate(canvas.width, 0);
            ctx.scale(-1, 1);
            
            ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
            
            const dataUrl = canvas.toDataURL('image/jpeg');
            imagePreview.src = dataUrl;
            imagePreviewContainer.classList.remove('hidden');
            
            // Convert to a File object to send to backend
            canvas.toBlob((blob) => {
                fileToScan = new File([blob], 'snapshot.jpg', { type: 'image/jpeg' });
            }, 'image/jpeg');
            
            stopCamera();
        });

        // 3. Initiate Scan (handles both file and camera)
        button.addEventListener('click', async () => {
            if (!fileToScan) { // Use the global variable
                showError("Please select an image file or take a snapshot first.");
                return;
            }

            resultsDiv.classList.add('hidden');
            errorDiv.classList.add('hidden');
            loader.classList.remove('hidden');
            imagePreviewContainer.classList.add('hidden'); // Hide preview on scan

            const formData = new FormData();
            formData.append('image', fileToScan); // Use the global file

            try {
                const response = await fetch('http://127.0.0.1:5000/recognize', {
                    method: 'POST',
                    body: formData
                });
                // ... (rest of the try block is unchanged)
                if (!response.ok) {
                    const errData = await response.json();
                    throw new Error(errData.error || `Server error: ${response.status}`);
                }

                const data = await response.json();

                resultText.textContent = `Recognized as: ${data.person} (Distance: ${data.distance})`;
                testImage.src = 'data:image/png;base64,' + data.test_image_b64;
                matchImage.src = 'data:image/png;base64,' + data.match_image_b64;
                
                loader.classList.add('hidden');
                resultsDiv.classList.remove('hidden');
                uploader.value = ''; // Clear file input
                fileToScan = null; // Clear the global file

            } catch (error) {
                console.error("Fetch error:", error);
                showError(`Error: ${error.message}. Is the Python server running?`);
            }
        });
        
        function showError(message) {
            // ... (unchanged)
            loader.classList.add('hidden');
            resultsDiv.classList.add('hidden');
            errorDiv.textContent = message;
            errorDiv.classList.remove('hidden');
        }

        // === Add Person Logic (with Multi-Preview) ===
        // ... (this entire section is unchanged)
        newImageUploader.addEventListener('change', () => {
            if (newImageUploader.files.length > 0) {
                newFilesList = newFilesList.concat(Array.from(newImageUploader.files));
                updateNewImagePreview();
            }
            newImageUploader.value = '';
        });

        function updateNewImagePreview() {
            if (newFilesList.length === 0) {
                newImagePreviewContainer.classList.add('hidden');
                return;
            }
            
            newImageGallery.innerHTML = '';
            newImageQueueHeader.textContent = `Image Queue (${newFilesList.length}):`;
            
            newFilesList.forEach((file, index) => {
                const reader = new FileReader();
                reader.onload = (e) => {
                    const imgContainer = document.createElement('div');
                    imgContainer.className = 'relative w-16 h-16';
                    imgContainer.innerHTML = `
                        <img src="${e.target.result}" alt="Preview ${index}" class="w-full h-full object-cover rounded-md border border-slate-700">
                        <button onclick="removeNewImage(${index})" class="absolute -top-2 -right-2 bg-red-600 text-white rounded-full w-5 h-5 text-xs flex items-center justify-center font-bold transition-transform hover:scale-110">&times;</button>
                    `;
                    newImageGallery.appendChild(imgContainer);
                };
                reader.readAsDataURL(file);
            });
            newImagePreviewContainer.classList.remove('hidden');
        }

        window.removeNewImage = function(index) {
            newFilesList.splice(index, 1);
            updateNewImagePreview();
        }
        
        clearQueueButton.addEventListener('click', () => {
             newFilesList = [];
             updateNewImagePreview();
        });

        addPersonButton.addEventListener('click', async () => {
            const name = personNameInput.value.trim();
            const files = newFilesList; 

            if (!name) {
                showAddMessage("Please enter an identity name.", 'error');
                return;
            }
            if (files.length === 0) {
                showAddMessage("Please select one or more images to upload.", 'error');
                return;
            }

            addLoader.classList.remove('hidden');
            addMessage.classList.add('hidden');

            const formData = new FormData();
            formData.append('personName', name);
            for (const file of files) {
                formData.append('image', file);
            }

            try {
                const response = await fetch('http://127.0.0.1:5000/add_person', {
                    method: 'POST',
                    body: formData
                });

                const data = await response.json();

                if (!response.ok) {
                    throw new Error(data.error || `Server error: ${response.status}`);
                }

                showAddMessage(data.message, 'success');
                personNameInput.value = '';
                newFilesList = [];
                updateNewImagePreview();

            } catch (error) {
                console.error("Add person error:", error);
                showAddMessage(`Error: ${error.message}`, 'error');
            } finally {
                addLoader.classList.add('hidden');
            }
        });

        function showAddMessage(message, type) {
            // ... (unchanged)
            addMessage.textContent = message;
            
            addMessage.classList.remove('bg-red-900/30', 'text-red-300', 'border-red-700');
            addMessage.classList.remove('bg-green-900/30', 'text-green-300', 'border-green-700');

            if (type === 'error') {
                addMessage.classList.add('bg-red-900/30', 'text-red-300', 'border-red-700');
            } else {
                addMessage.classList.add('bg-green-900/30', 'text-green-300', 'border-green-700');
            }
            
            addMessage.classList.remove('hidden');
        }

        // === Load Model Details Logic ===
        // ... (this entire section is unchanged)
        loadDetailsButton.addEventListener('click', async () => {
            detailsLoader.classList.remove('hidden');
            detailsError.classList.add('hidden');
            detailsResults.classList.add('hidden');
            loadDetailsButton.classList.add('hidden');

            try {
                const response = await fetch('http://127.0.0.1:5000/get_model_details', {
                    method: 'GET',
                });

                const data = await response.json();

                if (!response.ok) {
                    throw new Error(data.error || `Server error: ${response.status}`);
                }

                meanFaceImage.src = 'data:image/png;base64,' + data.mean_face_b64;

                eigenfacesContainer.innerHTML = '';
                data.eigenfaces_b64.forEach((b64String, index) => {
                    const eigenfaceDiv = document.createElement('div');
                    eigenfaceDiv.classList.add('w-1/3', 'md:w-1/6', 'p-1');
                    eigenfaceDiv.innerHTML = `
                        <img src="data:image/png;base64,${b64String}" alt="Eigenface ${index + 1}" class="w-full h-auto object-cover rounded-lg shadow-md border border-slate-700">
                        <p class="text-xs text-slate-400 mt-1">E-face ${index + 1}</p>
                    `;
                    eigenfacesContainer.appendChild(eigenfaceDiv);
                });

                detailsResults.classList.remove('hidden');

            } catch (error) {
                console.error("Load details error:", error);
                showDetailsError(`Error: ${error.message}. Is the Python server running?`);
                loadDetailsButton.classList.remove('hidden');
            } finally {
                detailsLoader.classList.add('hidden');
            }
        });

        function showDetailsError(message) {
            detailsError.textContent = message;
            detailsError.classList.remove('hidden');
        }

    </script>
</body>
</html>

